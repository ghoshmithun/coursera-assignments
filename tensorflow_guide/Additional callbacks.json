{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional callbacks\n",
    "\n",
    "In this reading we'll be looking at more of the inbuilt callbacks available in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again be using the sklearn diabetes dataset to demonstrate these callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the input and target variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build a simple model to fit to the data with our callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='mse',\n",
    "                optimizer=\"adam\",metrics=[\"mse\",\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler\n",
    "\n",
    "**Usage:** `tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)`\n",
    "\n",
    "The learning rate scheduler that we implemented in the previous reading as a custom callback is also available as a built in callback. \n",
    "\n",
    "As in our custom callback, the `LearningRateScheduler` in Keras takes a function `schedule` as an argument. \n",
    "\n",
    "This function `schedule` should take two arguments:\n",
    "* The current epoch (as an integer), and\n",
    "* The current learning rate,\n",
    "\n",
    "and return new learning rate for that epoch. \n",
    "\n",
    "The `LearningRateScheduler` also has an optional `verbose` argument, which prints information about the learning rate if it is set to 1.\n",
    "\n",
    "Let's see a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule function\n",
    "\n",
    "def lr_function(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_function, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use lambda functions to define your `schedule` given an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.3333333333333333.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.125.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.07692307692307693.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05555555555555555.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.043478260869565216.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.03571428571428571.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.030303030303030304.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.02631578947368421.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.023255813953488372.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.020833333333333332.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with a difference schedule\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda x:1/(3+5*x), verbose=1)], \n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV logger\n",
    "**Usage** `tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)`\n",
    "\n",
    "This callback streams the results from each epoch into a CSV file.\n",
    "The first line of the CSV file will be the names of pieces of information recorded on each subsequent line, beginning with the epoch and loss value. The values of metrics at the end of each epoch will also be recorded.\n",
    "\n",
    "The only compulsory argument is the `filename` for the log to be streamed to. This could also be a filepath.\n",
    "\n",
    "You can also specify the `separator` to be used between entries on each line.\n",
    "\n",
    "The `append` argument allows you the option to append your results to an existing file with the same name. This can be particularly useful if you are continuing training.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with a CSV logger\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.CSVLogger(\"results.csv\")], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information in the CSV file we have created using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26586.193256</td>\n",
       "      <td>143.77396</td>\n",
       "      <td>26586.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26492.404740</td>\n",
       "      <td>143.44408</td>\n",
       "      <td>26492.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26399.585121</td>\n",
       "      <td>143.11660</td>\n",
       "      <td>26399.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26307.509436</td>\n",
       "      <td>142.79604</td>\n",
       "      <td>26307.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26217.308077</td>\n",
       "      <td>142.48024</td>\n",
       "      <td>26217.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26127.783661</td>\n",
       "      <td>142.16873</td>\n",
       "      <td>26127.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26039.315668</td>\n",
       "      <td>141.86015</td>\n",
       "      <td>26039.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25953.938287</td>\n",
       "      <td>141.55734</td>\n",
       "      <td>25953.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25869.808087</td>\n",
       "      <td>141.26079</td>\n",
       "      <td>25869.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25786.101346</td>\n",
       "      <td>140.96638</td>\n",
       "      <td>25786.102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss        mae        mse\n",
       "epoch                                    \n",
       "0      26586.193256  143.77396  26586.191\n",
       "1      26492.404740  143.44408  26492.402\n",
       "2      26399.585121  143.11660  26399.584\n",
       "3      26307.509436  142.79604  26307.510\n",
       "4      26217.308077  142.48024  26217.307\n",
       "5      26127.783661  142.16873  26127.780\n",
       "6      26039.315668  141.86015  26039.314\n",
       "7      25953.938287  141.55734  25953.940\n",
       "8      25869.808087  141.26079  25869.807\n",
       "9      25786.101346  140.96638  25786.102"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"results.csv\", index_col='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda callbacks\n",
    "**Usage** `tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_begin=None, on_epoch_end=None, \n",
    "        on_batch_begin=None, on_batch_end=None, \n",
    "        on_train_begin=None, on_train_end=None)`\n",
    "\n",
    "Lambda callbacks are used to quickly define simple custom callbacks with the use of lambda functions.\n",
    "\n",
    "Each of the functions require some positional arguments.\n",
    "* `on_epoch_begin` and `on_epoch_end` expect two arguments: `epoch` and `logs`,\n",
    "* `on_batch_begin` and `on_batch_end` expect two arguments: `batch` and `logs` and\n",
    "* `on_train_begin` and `on_train_end` expect one argument: `logs`.\n",
    "\n",
    "Let's see an example of this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the epoch number at the beginning of each epoch\n",
    "\n",
    "epoch_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss at the end of each batch\n",
    "\n",
    "batch_loss_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform that training is finished\n",
    "\n",
    "train_finish_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_train_end=lambda logs: print('Training finished!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1!\n",
      "\n",
      " After batch 0, the loss is 23444.98.\n",
      "\n",
      " After batch 1, the loss is 28985.67.\n",
      "\n",
      " After batch 2, the loss is 24693.87.\n",
      "\n",
      " After batch 3, the loss is 25804.19.\n",
      "Starting Epoch 2!\n",
      "\n",
      " After batch 0, the loss is 27851.83.\n",
      "\n",
      " After batch 1, the loss is 24684.39.\n",
      "\n",
      " After batch 2, the loss is 26624.98.\n",
      "\n",
      " After batch 3, the loss is 23599.99.\n",
      "Starting Epoch 3!\n",
      "\n",
      " After batch 0, the loss is 27505.11.\n",
      "\n",
      " After batch 1, the loss is 25839.08.\n",
      "\n",
      " After batch 2, the loss is 27861.82.\n",
      "\n",
      " After batch 3, the loss is 21387.13.\n",
      "Starting Epoch 4!\n",
      "\n",
      " After batch 0, the loss is 24650.42.\n",
      "\n",
      " After batch 1, the loss is 27905.78.\n",
      "\n",
      " After batch 2, the loss is 27329.62.\n",
      "\n",
      " After batch 3, the loss is 22643.34.\n",
      "Starting Epoch 5!\n",
      "\n",
      " After batch 0, the loss is 26259.84.\n",
      "\n",
      " After batch 1, the loss is 26412.13.\n",
      "\n",
      " After batch 2, the loss is 26375.25.\n",
      "\n",
      " After batch 3, the loss is 23404.60.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the lambda callbacks\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=5, batch_size=100,\n",
    "                    callbacks=[epoch_callback, batch_loss_callback,train_finish_callback], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce learning rate on plateau\n",
    "**Usage** `tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.1, \n",
    "            patience=10, \n",
    "            verbose=0, \n",
    "            mode='auto', \n",
    "            min_delta=0.0001, \n",
    "            cooldown=0, \n",
    "            min_lr=0)`\n",
    "\n",
    "The `ReduceLROnPlateau` callback allows reduction of the learning rate when a metric has stopped improving. \n",
    "The arguments are similar to those used in the `EarlyStopping` callback.\n",
    "* The argument `monitor` is used to specify which metric to base the callback on.\n",
    "* The `factor` is the factor by which the learning rate decreases i.e., new_lr=factor*old_lr.\n",
    "* The `patience` is the number of epochs where there is no improvement on the monitored metric before the learning rate is reduced.\n",
    "* The `verbose` argument will produce progress messages when set to 1.\n",
    "* The `mode` determines whether the learning rate will decrease when the monitored quantity stops increasing (`max`) or decreasing (`min`). The `auto` setting causes the callback to infer the mode from the monitored quantity.\n",
    "* The `min_delta` is the smallest change in the monitored quantity to be deemed an improvement.\n",
    "* The `cooldown` is the number of epochs to wait after the learning rate is changed before the callback resumes normal operation.\n",
    "* The `min_lr` is a lower bound on the learning rate that the callback will produce.\n",
    "\n",
    "Let's examine a final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the ReduceLROnPlateau callback\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=100, batch_size=100,\n",
    "                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                        monitor=\"loss\",factor=0.2, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25604.554092</td>\n",
       "      <td>25604.554688</td>\n",
       "      <td>140.315475</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25579.444924</td>\n",
       "      <td>25579.445312</td>\n",
       "      <td>140.226105</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25554.332705</td>\n",
       "      <td>25554.332031</td>\n",
       "      <td>140.137161</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25529.539382</td>\n",
       "      <td>25529.537109</td>\n",
       "      <td>140.048309</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25504.604913</td>\n",
       "      <td>25504.605469</td>\n",
       "      <td>139.959671</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25479.749178</td>\n",
       "      <td>25479.748047</td>\n",
       "      <td>139.871216</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25455.035766</td>\n",
       "      <td>25455.037109</td>\n",
       "      <td>139.782837</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25430.330560</td>\n",
       "      <td>25430.330078</td>\n",
       "      <td>139.694504</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25405.773610</td>\n",
       "      <td>25405.773438</td>\n",
       "      <td>139.606323</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25381.327367</td>\n",
       "      <td>25381.328125</td>\n",
       "      <td>139.518005</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25356.455044</td>\n",
       "      <td>25356.453125</td>\n",
       "      <td>139.429977</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25332.351169</td>\n",
       "      <td>25332.349609</td>\n",
       "      <td>139.341995</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25307.515231</td>\n",
       "      <td>25307.515625</td>\n",
       "      <td>139.254074</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25283.182187</td>\n",
       "      <td>25283.183594</td>\n",
       "      <td>139.166458</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25258.640905</td>\n",
       "      <td>25258.640625</td>\n",
       "      <td>139.079010</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25234.282332</td>\n",
       "      <td>25234.281250</td>\n",
       "      <td>138.991898</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25210.511463</td>\n",
       "      <td>25210.511719</td>\n",
       "      <td>138.904358</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25185.828799</td>\n",
       "      <td>25185.828125</td>\n",
       "      <td>138.816940</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25161.955462</td>\n",
       "      <td>25161.955078</td>\n",
       "      <td>138.729691</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25137.624897</td>\n",
       "      <td>25137.625000</td>\n",
       "      <td>138.642441</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25113.566175</td>\n",
       "      <td>25113.564453</td>\n",
       "      <td>138.555313</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25089.207282</td>\n",
       "      <td>25089.208984</td>\n",
       "      <td>138.468552</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25065.487145</td>\n",
       "      <td>25065.488281</td>\n",
       "      <td>138.381760</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25041.318866</td>\n",
       "      <td>25041.320312</td>\n",
       "      <td>138.294968</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25017.546098</td>\n",
       "      <td>25017.546875</td>\n",
       "      <td>138.208252</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24993.541872</td>\n",
       "      <td>24993.541016</td>\n",
       "      <td>138.121552</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24969.687431</td>\n",
       "      <td>24969.687500</td>\n",
       "      <td>138.035004</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24945.852345</td>\n",
       "      <td>24945.851562</td>\n",
       "      <td>137.948547</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24922.034955</td>\n",
       "      <td>24922.035156</td>\n",
       "      <td>137.862198</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24898.544809</td>\n",
       "      <td>24898.544922</td>\n",
       "      <td>137.775635</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>23953.287980</td>\n",
       "      <td>23953.287109</td>\n",
       "      <td>134.302948</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>23930.914008</td>\n",
       "      <td>23930.914062</td>\n",
       "      <td>134.219284</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>23908.359852</td>\n",
       "      <td>23908.359375</td>\n",
       "      <td>134.135864</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>23886.258841</td>\n",
       "      <td>23886.259766</td>\n",
       "      <td>134.052414</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>23863.450006</td>\n",
       "      <td>23863.451172</td>\n",
       "      <td>133.969193</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>23841.637939</td>\n",
       "      <td>23841.636719</td>\n",
       "      <td>133.885986</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23819.398078</td>\n",
       "      <td>23819.398438</td>\n",
       "      <td>133.802536</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>23796.581229</td>\n",
       "      <td>23796.582031</td>\n",
       "      <td>133.719498</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>23774.791759</td>\n",
       "      <td>23774.791016</td>\n",
       "      <td>133.636627</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>23752.436388</td>\n",
       "      <td>23752.435547</td>\n",
       "      <td>133.553619</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>23730.335878</td>\n",
       "      <td>23730.335938</td>\n",
       "      <td>133.470734</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>23707.917944</td>\n",
       "      <td>23707.919922</td>\n",
       "      <td>133.388062</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>23686.167811</td>\n",
       "      <td>23686.167969</td>\n",
       "      <td>133.305313</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>23664.063322</td>\n",
       "      <td>23664.062500</td>\n",
       "      <td>133.222427</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>23642.196562</td>\n",
       "      <td>23642.197266</td>\n",
       "      <td>133.139404</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>23620.049030</td>\n",
       "      <td>23620.048828</td>\n",
       "      <td>133.056381</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>23598.110772</td>\n",
       "      <td>23598.111328</td>\n",
       "      <td>132.973526</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>23575.999223</td>\n",
       "      <td>23576.000000</td>\n",
       "      <td>132.890701</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>23554.125182</td>\n",
       "      <td>23554.125000</td>\n",
       "      <td>132.807938</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>23532.067434</td>\n",
       "      <td>23532.068359</td>\n",
       "      <td>132.725418</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>23510.143203</td>\n",
       "      <td>23510.144531</td>\n",
       "      <td>132.643036</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>23488.162675</td>\n",
       "      <td>23488.164062</td>\n",
       "      <td>132.560684</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>23466.755604</td>\n",
       "      <td>23466.755859</td>\n",
       "      <td>132.478241</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>23444.857845</td>\n",
       "      <td>23444.859375</td>\n",
       "      <td>132.395569</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>23422.685970</td>\n",
       "      <td>23422.685547</td>\n",
       "      <td>132.313293</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>23401.321877</td>\n",
       "      <td>23401.322266</td>\n",
       "      <td>132.231003</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>23379.184834</td>\n",
       "      <td>23379.183594</td>\n",
       "      <td>132.148804</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>23357.581210</td>\n",
       "      <td>23357.582031</td>\n",
       "      <td>132.066833</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>23335.953902</td>\n",
       "      <td>23335.955078</td>\n",
       "      <td>131.984756</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>23314.369839</td>\n",
       "      <td>23314.371094</td>\n",
       "      <td>131.902649</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss           mse         mae        lr\n",
       "0   25604.554092  25604.554688  140.315475  0.020833\n",
       "1   25579.444924  25579.445312  140.226105  0.020833\n",
       "2   25554.332705  25554.332031  140.137161  0.020833\n",
       "3   25529.539382  25529.537109  140.048309  0.020833\n",
       "4   25504.604913  25504.605469  139.959671  0.020833\n",
       "5   25479.749178  25479.748047  139.871216  0.020833\n",
       "6   25455.035766  25455.037109  139.782837  0.020833\n",
       "7   25430.330560  25430.330078  139.694504  0.020833\n",
       "8   25405.773610  25405.773438  139.606323  0.020833\n",
       "9   25381.327367  25381.328125  139.518005  0.020833\n",
       "10  25356.455044  25356.453125  139.429977  0.020833\n",
       "11  25332.351169  25332.349609  139.341995  0.020833\n",
       "12  25307.515231  25307.515625  139.254074  0.020833\n",
       "13  25283.182187  25283.183594  139.166458  0.020833\n",
       "14  25258.640905  25258.640625  139.079010  0.020833\n",
       "15  25234.282332  25234.281250  138.991898  0.020833\n",
       "16  25210.511463  25210.511719  138.904358  0.020833\n",
       "17  25185.828799  25185.828125  138.816940  0.020833\n",
       "18  25161.955462  25161.955078  138.729691  0.020833\n",
       "19  25137.624897  25137.625000  138.642441  0.020833\n",
       "20  25113.566175  25113.564453  138.555313  0.020833\n",
       "21  25089.207282  25089.208984  138.468552  0.020833\n",
       "22  25065.487145  25065.488281  138.381760  0.020833\n",
       "23  25041.318866  25041.320312  138.294968  0.020833\n",
       "24  25017.546098  25017.546875  138.208252  0.020833\n",
       "25  24993.541872  24993.541016  138.121552  0.020833\n",
       "26  24969.687431  24969.687500  138.035004  0.020833\n",
       "27  24945.852345  24945.851562  137.948547  0.020833\n",
       "28  24922.034955  24922.035156  137.862198  0.020833\n",
       "29  24898.544809  24898.544922  137.775635  0.020833\n",
       "..           ...           ...         ...       ...\n",
       "70  23953.287980  23953.287109  134.302948  0.020833\n",
       "71  23930.914008  23930.914062  134.219284  0.020833\n",
       "72  23908.359852  23908.359375  134.135864  0.020833\n",
       "73  23886.258841  23886.259766  134.052414  0.020833\n",
       "74  23863.450006  23863.451172  133.969193  0.020833\n",
       "75  23841.637939  23841.636719  133.885986  0.020833\n",
       "76  23819.398078  23819.398438  133.802536  0.020833\n",
       "77  23796.581229  23796.582031  133.719498  0.020833\n",
       "78  23774.791759  23774.791016  133.636627  0.020833\n",
       "79  23752.436388  23752.435547  133.553619  0.020833\n",
       "80  23730.335878  23730.335938  133.470734  0.020833\n",
       "81  23707.917944  23707.919922  133.388062  0.020833\n",
       "82  23686.167811  23686.167969  133.305313  0.020833\n",
       "83  23664.063322  23664.062500  133.222427  0.020833\n",
       "84  23642.196562  23642.197266  133.139404  0.020833\n",
       "85  23620.049030  23620.048828  133.056381  0.020833\n",
       "86  23598.110772  23598.111328  132.973526  0.020833\n",
       "87  23575.999223  23576.000000  132.890701  0.020833\n",
       "88  23554.125182  23554.125000  132.807938  0.020833\n",
       "89  23532.067434  23532.068359  132.725418  0.020833\n",
       "90  23510.143203  23510.144531  132.643036  0.020833\n",
       "91  23488.162675  23488.164062  132.560684  0.020833\n",
       "92  23466.755604  23466.755859  132.478241  0.020833\n",
       "93  23444.857845  23444.859375  132.395569  0.020833\n",
       "94  23422.685970  23422.685547  132.313293  0.020833\n",
       "95  23401.321877  23401.322266  132.231003  0.020833\n",
       "96  23379.184834  23379.183594  132.148804  0.020833\n",
       "97  23357.581210  23357.582031  132.066833  0.020833\n",
       "98  23335.953902  23335.955078  131.984756  0.020833\n",
       "99  23314.369839  23314.371094  131.902649  0.020833\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "* https://keras.io/callbacks/\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
